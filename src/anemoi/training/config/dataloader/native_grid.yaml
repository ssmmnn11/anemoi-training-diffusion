prefetch_factor: 2

num_workers:
  training: 8
  validation: 8
  test: 8
  predict: 8
batch_size:
  training: 2
  validation: 4
  test: 4
  predict: 4

# ============
# Default effective batch_size for training is 16
# For the o96 resolution, default per-gpu batch_size is 2 (8 gpus required)
# The global lr is calculated as:
# global_lr = local_lr * num_gpus_per_node * num_nodes / gpus_per_model
# Assuming a constant effective batch_size, any change in the per_gpu batch_size
# should come with a rescaling of the local_lr to keep a constant global_lr
# ============

# runs only N training batches [N = integer | null]
# if null then we run through all the batches
limit_batches:
  training: null
  validation: null
  test: 20
  predict: 20

# ============
# Dataloader definitions
# These follow the anemoi-datasets patterns
# You can make these as complicated for merging as you like
# See https://anemoi-datasets.readthedocs.io
# ============


dataset:
  observations: # for now observations must be on top level, will be removed
    multiple: # automatically pad the datasets with the min and max dates, will be renamed to zip
      - dataset: observations-ea-ofb-0001-2004-2023-combined-metar-v1
        frequency: 6h # this may become optional
        window: (-6, 0) # this may become optional
        rename_prefix: metar # this may become optional
        is_observations: true # will disappear
        select:
          - cos_latitude
          - sin_latitude
          - cos_longitude
          - sin_longitude
          - stalt
          - t2m_0
      - dataset: aifs-ea-an-oper-0001-mars-o96-1979-2022-6h-v6

training: ${dataloader.dataset}
validation: ${dataloader.dataset}
test: ${dataloader.dataset}
